{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **【AIに関する講義パートに関する基本方針】**\n",
        "\n",
        "AIの社会実装を巡る議論を行っていく大前提として、まずは今回より2回に分けて古典的な機械学習について復習します（その後、2回に分けて深層学習について同様に復習）。今後の予定を示すならば以下のとおりです。\n",
        "\n",
        "なお、pythonについては「自分で既にある程度学んでいること」を前提に議論を進めていきます。そのつもりでプログラミングの基礎については、厭うことなく自ら手を動かして覚えていきましょう。\n",
        "\n",
        "また全てgithub上にcolabのコードをアップロードしておきますので、そちらをダウンロードして受講するようにして下さい。\n",
        "\n",
        "各回の冒頭には前回の最後に課した課題について、colab上で解いたものを受講者自身で展開してもらい、プレゼンをしてもらいます。ChatGPTに「相談」してそのままコピーするのではなく、コードの細部に至るまで質問されたらば答えられるように準備をしておいてください。\n",
        "\n",
        "**（第7講（今回））**\n",
        "\n",
        "●古典的な機械学習の復習その1（回帰分析、k-means、ランダムフォレスト）\n",
        "\n",
        "**（第8回）**\n",
        "\n",
        "●古典的な機械学習の復習その2（ロジスティック回帰、サポートベクトルマシーン（SVM））\n",
        "\n",
        "**（第9回）**\n",
        "\n",
        "●深層学習の復習その1（畳込みニューラルネットワーク（CNN）、強化学習）\n",
        "\n",
        "**（第10回）**\n",
        "\n",
        "●深層学習の復習その2（再帰型ニューラルネットワーク、LSTM、自然言語処理の各種手法）\n",
        "\n",
        "第11講においては大規模言語モデル（LLM）としてgpt-4.1等を実際に用いつつ、AIエージェントの実装に向けた議論を展開します。"
      ],
      "metadata": {
        "id": "VeUpkxF1-1Ks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **【回帰分析（重回帰分析（Multiple Regression Analysis）】**"
      ],
      "metadata": {
        "id": "xGe1DkYQCW7L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C99LUfOB-t9v"
      },
      "outputs": [],
      "source": [
        "#まずは対象となるデータをダウンロードする。\n",
        "#具体的には、UC バークレー大学の UCI Machine Leaning Repository にて公開されている、「Wine Quality Data Set (ワインの品質)」の赤ワインのデータセット。\n",
        "\n",
        "!wget http://pythondatascience.plavox.info/wp-content/uploads/2016/07/winequality-red.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('winequality-red.csv', sep=';')\n",
        "df.head()\n",
        "\n",
        "#データはダウンロードしたらばまずはじっくり見てみる癖をつけること。"
      ],
      "metadata": {
        "id": "vCO92H5KC7Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[['density', 'pH']]\n",
        "y = df[['alcohol']]\n",
        "x1 = df[['density']]\n",
        "x2 = df[['pH']]\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bduVTN0vDKlQ",
        "outputId": "33d8ff22-53c6-44c7-dddf-21fa741bcba7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1599, 2)\n",
            "(1599, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ここで必要になってくるのが「正規化」と言う作業。\n",
        "#以下ではsklearn上のstandard scalerを用いてみる。\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "sscaler = preprocessing.StandardScaler()\n",
        "sscaler.fit(x)\n",
        "xss_sk = sscaler.transform(x)\n",
        "sscaler.fit(y)\n",
        "yss_sk = sscaler.transform(y)\n",
        "\n",
        "print(xss_sk)\n",
        "print(yss_sk)"
      ],
      "metadata": {
        "id": "HGYcZP5KEBkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【課題】**\n",
        "\n",
        "正規化には上記以外の方法もある。検索して探し、pytyon上で実際に演算を実行して示せ。"
      ],
      "metadata": {
        "id": "q0He0KJhEwx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#早速、上記の正規化したデータを用いて重回帰分析を施してみる。\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "model_lr_std = LinearRegression()\n",
        "model_lr_std.fit(xss_sk, yss_sk)\n",
        "\n",
        "print(model_lr_std.coef_)\n",
        "print(model_lr_std.intercept_)\n",
        "print(model_lr_std.score(xss_sk, yss_sk))\n",
        "\n",
        "#値はそれぞれ回帰係数、切片、決定係数を指す。\n",
        "#回帰係数は、説明変数が目的変数にどれくらい影響するかを表す係数。重回帰分析では各変数同士の影響度を示す。\n",
        "#回帰係数が大きいほど、説明変数が目的変数に強い影響を与えていることを示唆している。\n",
        "#決定係数は、0から1の間の値を取り、1に近いほど回帰式がデータに適合していることを示す。"
      ],
      "metadata": {
        "id": "74PCM6IVEuvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【課題】**\n",
        "\n",
        "上記で演算した結果をグラフ化することを試みよ。"
      ],
      "metadata": {
        "id": "0EllpSQQLGfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **【K-means法】**\n",
        "\n",
        "クラスタリングとは、ある集合を何らかの規則によって分類することを指す。機械学習においてクラスタリングは、「教師なし学習」に分類される。\n",
        "\n",
        "クラスタリングの計算方法はいくつかあるが、サンプル同士の類似性に基づいてグルーピングしている。クラスタリングの計算方法を大きく分類すると、「階層クラスタリング」と「非階層クラスタリング」の2つに分けられる。\n",
        "今回実装するK-means法は「非階層クラスタリング」に分類される。\n",
        "\n",
        "K-means法とはクラスターの平均を用いて、あらかじめ決められたクラスター数に分類手法を指す。K-means法のアルゴリズム概要は下記のとおり：\n",
        "\n",
        "**●クラスタの中心の初期値をk個決める**\n",
        "\n",
        "**●全てのサンプルとk個のクラスタとの中心距離を求め、最も近いクラスタに分類する**\n",
        "\n",
        "**●形成されたk個のクラスタの中心を求める**\n",
        "\n",
        "**●中心が変化しなくなるまで上記2番目と3番目の工程を繰り返す**"
      ],
      "metadata": {
        "id": "ixB18y1yLYWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#まずは必要なライブラリーをダウンロードする。\n",
        "\n",
        "# 必要なライブラリーのインストール\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 可視化\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# 正規化のためのクラス\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# k-means法に必要なものをインポート\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "AJvdG1TrLR23"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#次に有名なirisのデータセットをダウンロードする。\n",
        "\n",
        "# irisデータ\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# データ読み込み\n",
        "iris = load_iris()\n",
        "iris.keys()\n",
        "\n",
        "# データフレームに格納\n",
        "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df_iris['target'] = iris.target # アヤメの種類（正解ラベル）\n",
        "df_iris.head()"
      ],
      "metadata": {
        "id": "uE5vH8lONc8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【課題】**\n",
        "\n",
        "このデータセットについて、（1）petal_length, petal_widthの二変数で可視化し、さらに（2）それを散布図行列で可視化せよ。"
      ],
      "metadata": {
        "id": "xjzXYxl6Ntzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#次にエルボー法でクラスター数を決めてみる。\n",
        "#エルボー法はこちらを参照。https://zenn.dev/nakano_teppei/articles/e7164a385148b6\n",
        "\n",
        "# Elbow Method\n",
        "wcss = []\n",
        "\n",
        "for i in range(1, 10):\n",
        "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 30, random_state = 0)\n",
        "    kmeans.fit(df_iris.iloc[:, 2:4])\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "\n",
        "plt.plot(range(1, 10), wcss)\n",
        "plt.title('The elbow method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eS4SJaumOGIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#実際にライブラリーを使ってモデリングしてみる。\n",
        "\n",
        "# モデリング\n",
        "clf = KMeans(n_clusters=3, random_state=1)\n",
        "clf.fit(df_iris.iloc[:, 2:4])\n",
        "\n",
        "# 学習データのクラスタ番号\n",
        "clf.labels_\n",
        "\n",
        "# 未知データに対してクラスタ番号を付与\n",
        "# 今回は学習データに対して予測しているので、`clf.labels_` と同じ結果\n",
        "y_pred = clf.predict(df_iris.iloc[:, 2:4])\n",
        "y_pred\n",
        "\n",
        "# 実際の種類とクラスタリングの結果を比較\n",
        "fig, (ax1, ax2) = plt.subplots(figsize=(16, 4), ncols=2)\n",
        "\n",
        "# 実際の種類の分布\n",
        "ax1.scatter(df_iris['petal length (cm)'], df_iris['petal width (cm)'], c=df_iris.target, cmap=mpl.cm.jet)\n",
        "ax1.set_xlabel('petal_length')\n",
        "ax1.set_ylabel('petal_width')\n",
        "ax1.set_title('Actual')\n",
        "# クラスター分析で分類されたクラスタの分布\n",
        "ax2.scatter(df_iris['petal length (cm)'], df_iris['petal width (cm)'], c=y_pred, cmap=mpl.cm.jet)\n",
        "ax2.set_xlabel('petal_length')\n",
        "ax2.set_ylabel('petal_width')\n",
        "ax2.set_title('Predict')"
      ],
      "metadata": {
        "id": "UPly1Op8O1Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【課題】**\n",
        "\n",
        "K-means法と同じ様な呼称のアルゴリズムに「k近傍法」がある。これらはどの様に違うのかを簡単に説明した上で、実際のデータセットを用いつつ、python上に実装し、演算を実行せよ。"
      ],
      "metadata": {
        "id": "rUIQYnSkPWbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **【ランダムフォレスト】**\n",
        "\n",
        "機械学習における分類や回帰に用いられるアルゴリズムであり、決定木を複数作成し、その結果を統合することで高い精度を実現する。\n",
        "\n",
        "ブートストラップサンプリングと呼ばれる手法により、データセットからランダムにサンプルを取り出し、それぞれのサンプルで決定木を構築する。\n",
        "\n",
        "最後に、各決定木の予測結果を多数決（分類）または平均（回帰）を取ることで、最終的な予測を行う。"
      ],
      "metadata": {
        "id": "J_MghsahPlLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#まずは必要なライブラリーをダウンロードする。\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "0nkobNIJQQa8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#今回はオリジナルなデータをデータフレームに格納し、処理していく。\n",
        "#従業員の特徴（年齢、職務経験、部署）に基づいて離職のリスクを予測する。\n",
        "\n",
        "# サンプルデータ\n",
        "data = {\n",
        "    '年齢': [25, 45, 35, 50, 23, 40, 28, 50, 60, 36, 32, 25, 29, 41, 50, 38, 27, 46, 34, 49],\n",
        "    '職務経験（年数）': [3, 20, 10, 30, 2, 15, 5, 29, 35, 11, 8, 3, 6, 18, 28, 12, 4, 21, 9, 25],\n",
        "    '部署': ['営業', '技術', '人事', '技術', '営業', '人事', '営業', '技術', '営業', '技術', '人事', '営業', '営業', '技術', '人事', '技術', '営業', '技術', '人事', '技術'],\n",
        "    '離職': [0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1]\n",
        "}\n",
        "\n",
        "dataset = pd.DataFrame(data)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "pUtrO6CyQmwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 特徴量とラベルに分割\n",
        "X = pd.get_dummies(dataset.iloc[:, :-1])\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# トレーニングデータとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "-wyuVCEiQyyu"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ランダムフォレストモデルのトレーニング\n",
        "classifier = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# テストデータで予測\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# 混同行列の作成\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# 精度の計算\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy: .2f}')"
      ],
      "metadata": {
        "id": "dQlxOZp0Q9dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【課題】**\n",
        "\n",
        "classifier = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0)のコードが何を意味しているのかを調べて、説明せよ。"
      ],
      "metadata": {
        "id": "hdRe9vXbRSIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#部分的な可視化を図る。\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# ランダムフォレストの最初の決定木を可視化\n",
        "estimator = classifier.estimators_[0]\n",
        "\n",
        "# クラス名に対応する文字列のリストを作成\n",
        "# classifier.classes_が返すのは [0, 1] なので、それぞれに対応する文字列を定義\n",
        "class_labels = ['Not Left', 'Left']\n",
        "\n",
        "dot_data = export_graphviz(\n",
        "    estimator,\n",
        "    out_file=None,\n",
        "    feature_names=X.columns,\n",
        "    class_names=class_labels,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"random_forest_tree\")\n",
        "\n",
        "# グラフを表示\n",
        "graph"
      ],
      "metadata": {
        "id": "UdFtV56SSBCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【課題】**\n",
        "\n",
        "他の決定木を用いてその結果をグラフ化し、それが何を示しているのかについて解釈を示せ。"
      ],
      "metadata": {
        "id": "6VaAYLvbS8ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **【統合課題】**\n",
        "\n",
        "今回学んだ3つ（4つ）のアルゴリズムの中で1つを選択し、他方で現実社会において公開されているデータを別途入手し、分類問題を解くコーディングを行え（colab上のpythonを用いて）。なおその際、可能な限り可視化を試み、より説得力のある形でプレゼンテーションを行うこと。\n",
        "\n",
        "なお課題提出はipynbファイルで次回講義開始前までにslack上で行い、講義に際してはオンライン上で実行して結果を示すこと。"
      ],
      "metadata": {
        "id": "C_7JVJ-lTrss"
      }
    }
  ]
}